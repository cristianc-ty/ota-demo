{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# OTA Demo\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyspark.sql\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configuration\n",
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# environment archive created with conda pack\n",
    "ARCHIVES_PATH = \"hdfs:///user/metareview/ota_demo_support/ota_demo_env.tar.gz#cluster_venv\"\n",
    "# take the latest date form the S3 bucket\n",
    "META_REVIEW_S3_URI = \"s3a://trustyou-api/meta-review/2021-02-22_23-34-19/*.jsonl.gz\"\n",
    "HOTEL_S3_URI = \"s3a://trustyou-api/hotels/2021-02-21_00-00-00.jsonl.gz\"\n",
    "META_REVIEW_DUMP_PATH = \"hdfs:///user/metareview/ota_demo/meta_review_dump.orc\"\n",
    "HOTEL_DUMP_PATH = \"hdfs:///user/metareview/ota_demo/hotel_dump.orc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = \"./cluster_venv/bin/python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.master(\"yarn\") \\\n",
    "        .appName(\"OTA Demo Sample\") \\\n",
    "        .config(\"spark.executor.memory\", \"3g\") \\\n",
    "        .config(\"spark.executor.cores\", \"3\") \\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\") \\\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", \"4\") \\\n",
    "        .config(\"spark.sql.orc.filterPushdown\", \"false\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY\")) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_KEY\")) \\\n",
    "        .config(\"spark.yarn.dist.archives\", ARCHIVES_PATH) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:postgresql://{server}:{port}/{db}?user={user}&password={pw}\".format(\n",
    "    server=\"\",\n",
    "    port=\"5432\",\n",
    "    db=\"ota_demo\",\n",
    "    user=\"ota_demo\",\n",
    "    pw=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extract Meta-Review Data\n",
    "### Run the dump once then after that use the cache file\n",
    "#### Dump from S3, Restrict to one localisation version (here `en`) only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "meta_review_df = spark.read.json(META_REVIEW_S3_URI, samplingRatio=0.0001, mode=\"FAILFAST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_review_df = meta_review_df.where(F.col(\"lang\") == \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Write to HDFS cache file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "meta_review_df.write.orc(META_REVIEW_DUMP_PATH, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extract Hotel Data\n",
    "### Run the dump once then after that use the cache file\n",
    "#### Dump from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hotel_df = spark.read.json(HOTEL_S3_URI, samplingRatio=0.001, mode=\"FAILFAST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Write to HDFS cache file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hotel_df.write.orc(HOTEL_DUMP_PATH, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read in the DataFrames from the HDFS cache file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "meta_review_df = spark.read.orc(META_REVIEW_DUMP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_df = spark.read.orc(HOTEL_DUMP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Works around WIDGET-3334, remove once it is fixed\n",
    "hotel_df = hotel_df.drop_duplicates([\"ty_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Hotel Search Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add city and coordiantes to meta-review DFs by joining with hotel DF\n",
    "In practice, it's advisable to use your own hotel database instead. You could for example use the the address data in the hotel dump to map between your hotel database and our portfolio and then store the `ty_id` somewhere in your database to have a reference between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_review_with_hotel_df = meta_review_df.join(hotel_df, on=\"ty_id\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Flatten the data\n",
    "We are aiming for a schema with columns \n",
    "* `ty_id` - the unique identifier for each property\n",
    "* `trip_type` - indicating an applied filter by trip type, one of `all` (for unfiltered data), `solo`, `couple`, `family`, `business`. The filtered Meta-Reviews can be found in `trip_type_meta_review_list`.\n",
    "* `language` - indicating an applied filter by language, either `all` (for unfiltered data) or a two-character ISO language code. The filtered Meta-Reviews can be found in `language_meta_review_list`.\n",
    "* `city` - we got this by joining with the hotel bucket\n",
    "* `country` - we got this by joining with the hotel bucket\n",
    "* `latitude` - we got this by joining with the hotel bucket (in the `lat_lng` field)\n",
    "* `longitude` - we got this by joining with the hotel bucket (in the `lat_lng` field)\n",
    "* `datapoint` - we store here the category ID for categories from the `category_list` of for hotel_types from the `hotel_type_list` or the value `oall` for the overall datapoints from `summary`\n",
    "* `score` - the score of the `datapoint`\n",
    "* `review_count` - the number of reviews providing data for the `score` of the `datapoint`\n",
    "\n",
    "That means, we have to explode the dataframe on the different lists of filtered meta-reviews and then on `category_list` and `hotel_type_list` to get the different datapoints all in separate rows. For `category_list` we additionally want to extract `sub_category_list`, treating them the same as top-level categories for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unnest the different filtered meta-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the trip type-filtered meta-reviews, only keeping the data points we are interested in\n",
    "meta_review_df_tt_filter = meta_review_with_hotel_df \\\n",
    "    .select(\"ty_id\", \"city\", \"country\", \"lat_lng\", F.explode(\"trip_type_meta_review_list\").alias(\"filtered_mr\")) \\\n",
    "    .select(\"ty_id\", \"city\", \"country\", \"lat_lng\", \"filtered_mr.filter.trip_type\", \"filtered_mr.filter.language\", \"filtered_mr.category_list\", \"filtered_mr.summary.score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the language-filtered meta-reviews. We don't yet remove unneeded data points because...\n",
    "meta_review_df_lang_filter = meta_review_with_hotel_df \\\n",
    "    .select(\"ty_id\", \"city\", \"country\", \"lat_lng\", F.explode(\"language_meta_review_list\").alias(\"filtered_mr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...we first have to extract the meta-reviews which have both a trip type and a language filter\n",
    "meta_review_df_both_filters = meta_review_df_lang_filter \\\n",
    "    .select(\"ty_id\", \"city\", \"country\", \"lat_lng\", F.explode(\"filtered_mr.trip_type_meta_review_list\").alias(\"filtered_mr\")) \\\n",
    "    .select(\"ty_id\", \"city\", \"country\", \"lat_lng\", \"filtered_mr.filter.trip_type\", \"filtered_mr.filter.language\", \"filtered_mr.category_list\", \"filtered_mr.summary.score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we also format the meta-reviews with only a language filter in the way we want\n",
    "meta_review_df_lang_filter = meta_review_df_lang_filter.select(\"ty_id\", \"city\", \"country\", \"lat_lng\", \"filtered_mr.filter.trip_type\", \"filtered_mr.filter.language\", \"filtered_mr.category_list\", \"filtered_mr.summary.score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also get the unfiltered meta-reviews into the same format\n",
    "unfiltered_df_categories = meta_review_with_hotel_df.select(\n",
    "    \"ty_id\", \"city\", \"country\", \"lat_lng\", F.lit(\"all\").alias(\"trip_type\"), F.lit(\"all\").alias(\"language\"), \"category_list\", \"summary.score\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have all of the dataframes with different filter combinations in the same format now, we can merge them together \n",
    "all_filters_df = meta_review_df_tt_filter \\\n",
    "    .union(meta_review_df_lang_filter) \\\n",
    "    .union(meta_review_df_both_filters) \\\n",
    "    .union(unfiltered_df_categories) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unnest the different datapoints we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we extract the top-level categories from `category_list`\n",
    "category_df_exploded = all_filters_df.select(\n",
    "    \"ty_id\", \"city\", \"country\", \"lat_lng\", \"trip_type\", \"language\", F.explode(\"category_list\").alias(\"category\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we extract the sub-level categories from the top-level categories we just got before\n",
    "sub_category_df = category_df_exploded.select(\n",
    "    \"ty_id\", \"city\", \"country\", \"lat_lng\", \"trip_type\", \"language\", F.explode(\"category.sub_category_list\").alias(\"category\")\n",
    ").select(\n",
    "    \"ty_id\", \"city\", \"country\", \"lat_lng\", \"trip_type\", \"language\", F.col(\"category.category_id\").alias(\"datapoint\"), F.col(\"category.score\").alias(\"score\"), F.col(\"category.review_count\").alias(\"review_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since a sub-category can count to multiple top-level categories, there will be duplicates in the sub category df.\n",
    "# we drop them here.\n",
    "distinct_sub_category_df = sub_category_df.drop_duplicates([\"ty_id\", \"trip_type\", \"language\", \"datapoint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assure category_df has same format as sub_category_df\n",
    "category_df = category_df_exploded.select(\n",
    "    \"ty_id\", \"city\", \"country\", \"lat_lng\", \"trip_type\", \"language\", F.col(\"category.category_id\").alias(\"datapoint\"), F.col(\"category.score\").alias(\"score\"), F.col(\"category.review_count\").alias(\"review_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since they are now in the same format, we can merge the top-level and sub-level category DFs for the next steps\n",
    "combined_category_df = category_df.union(distinct_sub_category_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we get the hotel type datapoints. They work in a similar way as categories, but only exist for the unfiltered MR.\n",
    "hotel_type_df = meta_review_with_hotel_df.select(\n",
    "    \"ty_id\", \"city\", \"country\", \"lat_lng\", F.lit(\"all\").alias(\"trip_type\"), F.lit(\"all\").alias(\"language\"), F.explode(\"hotel_type_list\").alias(\"htype\")\n",
    ").select(\n",
    "    \"ty_id\", \"city\", \"country\", \"lat_lng\", F.lit(\"all\").alias(\"trip_type\"), F.lit(\"all\").alias(\"language\"), F.col(\"htype.category_id\").alias(\"datapoint\"), F.col(\"htype.score\").alias(\"score\"), F.lit(0).alias(\"review_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the final datapoint, we get a dataframe that includes the overall score and put it into the right format\n",
    "overall_score_df = all_filters_df.select(\n",
    "     \"ty_id\", \"city\", \"country\", \"lat_lng\", \"trip_type\", \"language\", F.lit(\"oall\").alias(\"datapoint\"), \"score\", F.lit(0).alias(\"review_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have now collected all datapoints we are interested in. We merge the different dataframes and, as a final step,\n",
    "# split `lat_lng` into two different columns\n",
    "merged_df = combined_category_df.union(overall_score_df).union(hotel_type_df).select(\n",
    "    F.col(\"ty_id\"), F.col(\"trip_type\"), F.col(\"language\"),\n",
    "    F.col(\"city\"), F.col(\"country\"), F.col(\"lat_lng\").getItem(0).alias(\"latitude\"), F.col(\"lat_lng\").getItem(1).alias(\"longitude\"),\n",
    "    F.col(\"datapoint\"), F.col(\"score\").cast(T.FloatType()), F.col(\"review_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the resulting dataframe we can now write to the PostgreSQL table. Note that this table was already created before\n",
    "# which is important since it allows for a finer-grained definition of the schema than when Spark creates it.\n",
    "merged_df.write.mode(\"overwrite\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"cluster_search\") \\\n",
    "    .option(\"truncate\", \"true\") \\\n",
    "    .option(\"stringtype\", \"unspecified\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City Search Backend\n",
    "#### Preparing a backend for search of the different cities. We use the hotel dump and group by city and country, counting the number of distinct clusters in each. \n",
    "Note: This is just for demo purposes. In practice, you will want to use your own hotel database instead for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_hotel_df = meta_review_with_hotel_df.groupby(\"city\", \"country\").agg(\n",
    "    F.countDistinct(F.col(\"ty_id\")).cast(T.ShortType()).alias(\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_hotel_df.write.mode(\"overwrite\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"city_search\") \\\n",
    "    .option(\"truncate\", \"true\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Shutdown Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
